<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Yuhao Ding </title>
<meta name="google-site-verification" content="MGZY7H6crNnv47GgWSFkuK5J6QkydGQkd-97rrlrB9U" />
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="publication.html">Publication</a></div>
<div class="menu-item"><a href="awards.html">Awards</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Yuhao Ding </h1>
</div>
<table class="imgtable"><tr><td>
<img src="photos/photo_Yuhao.jpeg" alt="alt text" width="250px" height="260px" />&nbsp;</td>
<td align="left"><p>Fourth-year PhD student,<br /> </p>
<p><a href="https://ieor.berkeley.edu/">Industrial Engineering and Operations Research</a>, <br /></p>
<p><a href="https://www.berkeley.edu/">University of California, Berkeley</a>,<br /></p>
<p><b>Advisor</b>: <a href="https://lavaei.ieor.berkeley.edu/">Javad Lavaei</a>,<br /></p>
<p><b>Email</b>: <i>yuhao_ding</i> [@] berkeley [DOT] edu, <br /></p>
<p><b><a href="cv/CV_Yuhao.pdf">CV</a></b></p>
</td></tr></table>
<h2>Brief Biography</h2>
<p>Yuhao Ding works on the interdisciplinary problems in reinforcement learning, optimization theory and control theory.  </p>
<h2>Recent News </h2>
<ul>
<li><p>Oct 2021: I gave a guest lecture on model-free reinforcement learning for IEOR 268 Applied Dynamic Programming at UC Berkeley.</p>
</li>
<li><p>Oct 2021: I gave a talk for the session on Recent Advances in Data Efficient Reinforcement Learning with Policy Gradient Methods at the 2021 INFORMS Annual Meeting.</p>
</li>
<li><p>Oct 2021: Two new papers posted on arXiv. In these papers, we derive the first set of global convergence results for stochastic policy gradient methods with <a href="https://arxiv.org/abs/2110.10116">momentum</a> and <a href="https://arxiv.org/abs/2110.10117">entropy</a>.</p>
</li>
<li><p>Oct 2021: New paper on constrained Markov decision processes: <a href="https://arxiv.org/abs/2110.08923">A Dual Approach to Constrained Markov Decision Processes with Entropy Regularization</a>.</p>
</li>
<li><p>July 2021: The paper <a href="pubs/2021-structured-online-opt.pdf">Structured Projection-free Online Convex Optimization with Multi-point Bandit Feedback</a> to appear in 60th Conference on Decision and Control.</p>
</li>
<li><p>June 2021: The paper <a href="pubs/2021-time-variation-TAC.pdf">Time-variation in online nonconvex optimization enables escaping from spurious local minima</a> has been conditionally accepted for IEEE Transactions on Automatic Control.</p>
</li>
<li><p>May 2021: I started the research internship at Microsoft research working with <a href="https://www.microsoft.com/en-us/research/people/emrek/">Emre Kiciman</a>.</p>
</li>
<li><p>April 2021: The paper <a href="pubs/2020-on-absence-TAC.pdf">On the absence of spurious local trajectories in time-varying nonconvex optimization</a> has been conditionally accepted for IEEE Transactions on Automatic Control.</p>
</li>
<li><p>January 2021: The paper <a href="https://ieeexplore.ieee.org/document/9483303">Escaping spurious local minimum trajectories in online time-varying nonconvex optimization</a> has been selected as a best student paper finalist for 2021 American Control Conference.</p>
</li>
</ul>
</td>
</tr>
</table>
</body>
</html>
